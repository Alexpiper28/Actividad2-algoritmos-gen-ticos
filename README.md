
# 1. Algoritmo Gen√©tico para Selecci√≥n de Caracter√≠sticas

Este notebook implementa un **algoritmo gen√©tico** para realizar la **selecci√≥n de caracter√≠sticas** utilizando el conjunto de datos **c√°ncer de mama** de `sklearn`. El objetivo es seleccionar las caracter√≠sticas m√°s relevantes para predecir si un tumor es maligno o benigno. Un **√°rbol de decisi√≥n** es utilizado como clasificador y se emplea **validaci√≥n cruzada** para evaluar la aptitud de las soluciones generadas por el algoritmo gen√©tico.

## Librer√≠as Importadas

- **`random`**: Para manejar la aleatoriedad en el algoritmo gen√©tico.
- **`numpy`**: Para manejar eficientemente arrays y matrices.
- **`sklearn.datasets`**: Para cargar el dataset de c√°ncer de mama.
- **`sklearn.model_selection`**: Para aplicar validaci√≥n cruzada con el clasificador.
- **`sklearn.tree`**: Para usar un clasificador de √°rbol de decisi√≥n.
- **`matplotlib.pyplot`**: Para graficar el progreso del algoritmo a trav√©s de las generaciones.

## Conjunto de Datos

El conjunto de datos de **c√°ncer de mama** contiene 30 caracter√≠sticas relacionadas con medidas de las c√©lulas tumorales. El objetivo es predecir si el tumor es maligno o benigno. Las caracter√≠sticas incluyen el radio, la textura, el per√≠metro, el √°rea, la suavidad, entre otras.

## Funcionamiento del Algoritmo Gen√©tico

- **Selecci√≥n**: Se seleccionan individuos basados en su aptitud para formar la siguiente generaci√≥n.
- **Cruzamiento (Crossover)**: Los individuos seleccionados intercambian partes de sus caracter√≠sticas para crear nuevas soluciones.
- **Mutaci√≥n**: Se introducen cambios aleatorios para mantener la diversidad en la poblaci√≥n.

## Evaluaci√≥n

La aptitud de cada individuo se eval√∫a mediante un **√°rbol de decisi√≥n** entrenado con las caracter√≠sticas seleccionadas y validado mediante **validaci√≥n cruzada**.

---

Este algoritmo gen√©tico permite identificar las mejores caracter√≠sticas para mejorar el rendimiento de un clasificador en tareas de clasificaci√≥n, como la detecci√≥n de tumores malignos en el c√°ncer de mama.


# 2. Optimizaci√≥n de Hiperpar√°metros con Algoritmos Gen√©ticos (PyTorch + DEAP)

Este repositorio demuestra c√≥mo optimizar hiperpar√°metros de un modelo sencillo de clasificaci√≥n para MNIST usando un Algoritmo Gen√©tico (GA) implementado con DEAP y PyTorch.

üß≠Objetivo
Buscar autom√°ticamente una combinaci√≥n de hiperpar√°metros (tasa de aprendizaje, tama√±o de lote y optimizador) que maximice la precisi√≥n en el conjunto de prueba.

üß©Resumen de la soluci√≥n
Modelo: Regresi√≥n log√≠stica (capa lineal + softmax impl√≠cito v√≠a CrossEntropyLoss).

Datos: MNIST (60k train / 10k test), im√°genes aplanadas a 784 caracter√≠sticas y escaladas a [0,1].

Dispositivo: Usa GPU (CUDA) si est√° disponible; si no, CPU.

GA (DEAP):

Individuo = [lr: float, batch_size_idx: int, optimizer_idx: int].
Espacio de b√∫squeda: lr ‚àà [1e-4, 1e-1], batch_size ‚àà {32, 64, 128, 256}, optimizer ‚àà {Adam, SGD}.
Operadores: cruce de dos puntos, torneo para selecci√≥n, mutaci√≥n personalizada por gen.
Fitness: precisi√≥n en test despu√©s de entrenar por 20 √©pocas.
Salida: mejor individuo, sus hiperpar√°metros mapeados y precisi√≥n alcanzada; entrenamiento final extendido (100 √©pocas) con los mejores hiperpar√°metros.

üõ†Ô∏è Requisitos
Python 3.9+ (recomendado)
PyTorch
torchvision
numpy
deap
Instalaci√≥n
Crear y activar entorno (opcional)
python -m venv .venv
 Windows: .venv\Scripts\activate
macOS/Linux:
source .venv/bin/activate

Instalar dependencias
pip install torch torchvision numpy deap
Nota: Instala la versi√≥n de torch/torchvision adecuada a tu sistema y CUDA. Consulta la gu√≠a oficial de PyTorch.

üìÅ Estructura del script
Carga y preparaci√≥n de datos

Descarga MNIST con torchvision.datasets.MNIST.
Aplana im√°genes a vectores de 784 y normaliza a [0,1].
Optimizaci√≥n: se mueven x_train, y_train, x_test, y_test una sola vez al dispositivo (GPU/CPU).
Modelo

SoftmaxModel: capa totalmente conectada Linear(784 ‚Üí 10).
La CrossEntropyLoss aplica log_softmax internamente.
Funci√≥n de aptitud (evaluate_hyperparameters)

Mapea genes del individuo a hiperpar√°metros reales.
Crea DataLoader seg√∫n batch_size.
Entrena por 20 √©pocas.
Eval√∫a precisi√≥n en x_test/y_test. Devuelve una tupla (accuracy,) como exige DEAP.
Configuraci√≥n DEAP

creator.FitnessMax(weights=(1.0,)) ‚Üí maximizar precisi√≥n.
Individuo con 3 genes: lr, batch_size_idx, optimizer_idx.
Operadores: cxTwoPoint, selTournament(tournsize=3), mutate_hyperparameters(indpb=0.2).
Ejecuci√≥n del GA

Par√°metros por defecto: POP_SIZE=20, CXPB=0.5, MUTPB=0.2, NGEN=10.
algorithms.eaSimple con estad√≠sticas (avg, std, min, max) y Hall of Fame.
Entrenamiento final

Reconstruye y entrena el modelo con los mejores hiperpar√°metros por 100 √©pocas.
Reporta precisi√≥n final en test.
‚ñ∂Ô∏è C√≥mo ejecutar
Guarda el script como ga_hpo_mnist.py y ejecuta:

python ga_hpo_mnist.py
Salida esperada (aprox.):

Log del GA por generaci√≥n con estad√≠sticas.
Mejor individuo y su mapeo a hiperpar√°metros.
Entrenamiento final y precisi√≥n en test.
Tiempo de ejecuci√≥n: depende del dispositivo (GPU vs CPU) y de POP_SIZE, NGEN y epochs (20 en evaluaci√≥n, 100 en entrenamiento final).

‚öôÔ∏è Hiperpar√°metros del GA
Tama√±o de poblaci√≥n (POP_SIZE): m√°s grande ‚Üí mejor exploraci√≥n (mayor costo).
Probabilidad de cruce (CXPB): control del recombinado.
Probabilidad de mutaci√≥n (MUTPB): evita estancamiento en √≥ptimos locales.
Generaciones (NGEN): m√°s iteraciones ‚Üí convergencia m√°s robusta.
Torneo (tournsize): presi√≥n selectiva; valores altos aceleran convergencia, pero pueden reducir diversidad.
Sugerencias:

Comienza con POP_SIZE entre 20‚Äì50 y NGEN entre 10‚Äì30.
Ajusta epochs de la evaluaci√≥n (20) para equilibrar fidelidad del fitness vs tiempo.
üß™ M√©trica de desempe√±o
Precisi√≥n en test.
Posibles extensiones: separar train/valid/test y usar precisi√≥n en validaci√≥n como fitness para evitar sobreajuste a test.
üîÅ Reproducibilidad
Para reproducibilidad, puedes fijar semillas:

import random, numpy as np, torch
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
Opcional: comportamiento determinista (puede ralentizar)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
Inserta esto al inicio del script, tras las importaciones.

C√≥mo extender el espacio de b√∫squeda
1) A√±adir nuevas opciones de batch_size
batch_size_options = [16, 32, 64, 128, 256, 512]
Y cambia los l√≠mites de random.randint para el gen del √≠ndice
toolbox.register("attr_batch_size", random.randint, 0, len(batch_size_options) - 1)
2) Incluir nuevas tasas de aprendizaje (mismo rango)
Ajusta los l√≠mites de random.uniform:

toolbox.register("attr_lr", random.uniform, 1e-5, 5e-1)
3) Probar m√°s optimizadores
optimizer_options = ['Adam', 'SGD', 'RMSprop', 'AdamW']
Cambia l√≠mites del √≠ndice y la selecci√≥n al crear el optimizador
4) Cambiar el modelo
Sustituye SoftmaxModel por una red m√°s profunda:

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(784, 256), nn.ReLU(),
            nn.Linear(256, 128), nn.ReLU(),
            nn.Linear(128, 10)
        )
    def forward(self, x):
        return self.net(x)
Recuerda mover el modelo a device y, si cambias la arquitectura, considera a√±adir √©pocas en la evaluaci√≥n.

Buenas pr√°cticas y notas de rendimiento
Mover tensores al dispositivo una sola vez (ya aplicado) reduce el overhead de copia.
Evita sobreajuste al conjunto de test: usa un split de validaci√≥n dentro de evaluate_hyperparameters y reserva test solo para el final.
Parada temprana (early stopping) durante la evaluaci√≥n puede acelerar el GA si no se observa mejora.
Penalizaci√≥n por complejidad: cuando uses modelos m√°s grandes, puedes restar una peque√±a penalizaci√≥n al fitness para favorecer modelos simples.
Caching (avanzado): guarda evaluaciones repetidas de individuos id√©nticos.
Soluci√≥n de problemas (FAQ)
Q1. ModuleNotFoundError: No module named 'deap' Instala con pip install deap.

Q2. Se agota la memoria en GPU Reduce batch_size o usa CPU (CUDA no disponible) y/o disminuye POP_SIZE/NGEN.

Q3. El GA no mejora Incrementa MUTPB o CXPB, o baja tournsize para aumentar diversidad; ampl√≠a el rango de lr.

Q4. Ejecuci√≥n muy lenta Baja epochs en la funci√≥n de aptitud, reduce POP_SIZE/NGEN, o usa un modelo m√°s peque√±o.

Detalles de implementaci√≥n clave del script
Individuo: lista con 3 genes [(float) lr, (int) batch_size_idx, (int) optimizer_idx].
Mutaci√≥n por gen con probabilidad indpb=0.2.
Evaluaci√≥n: entrenamiento in situ del modelo por 20 √©pocas y c√°lculo directo de precisi√≥n.
Hall of Fame (hof) guarda el mejor individuo global.
Estad√≠sticas (DEAP tools.Statistics) para seguimiento del progreso por generaci√≥n.

Ejemplo de salida (formato)
--- Iniciando la optimizaci√≥n de hiperpar√°metros con Algoritmo Gen√©tico ---
gen nevals  avg std min max
0   20  0.91    0.03    0.85    0.94
1   10  0.92    0.02    0.88    0.95
...
--- Optimizaci√≥n Finalizada ---
Mejor individuo encontrado: [0.0031, 2, 0]
Mejores hiperpar√°metros: {"learning_rate": 0.0031, "batch_size": 128, "optimizer": "Adam"}
Mejor precisi√≥n de validaci√≥n durante la HPO: 0.9530
--- Entrenando modelo final con los mejores hiperpar√°metros ---
√âpoca final [100/100], P√©rdida: 0.3124
Precisi√≥n final en el conjunto de prueba: 0.9620
Los valores son ilustrativos; variar√°n seg√∫n semillas, hardware y par√°metros del GA.

Roadmap de mejoras
A√±adir conjunto de validaci√≥n dentro de evaluate_hyperparameters.
Soportar m√°s hiperpar√°metros (momentum, weight decay, scheduler, capas ocultas, etc.).
Implementar paralelizaci√≥n de evaluaciones (DEAP + multiprocessing o joblib).
Incorporar early stopping y K-fold.
Guardar el mejor modelo y registros (CSV/JSON) de todas las evaluaciones.

Atribuciones
PyTorch
DEAP
MNIST dataset
Cita
Si usas este ejemplo en tu trabajo, puedes citarlo como:

"Optimizaci√≥n de hiperpar√°metros con Algoritmo Gen√©tico usando PyTorch y DEAP para MNIST (2025)".

# 3. Neuroevolution: Optimizaci√≥n de Redes Neuronales con Algoritmos Gen√©ticos

Este repositorio contiene ejemplos de **Neuroevolution**, es decir, la optimizaci√≥n autom√°tica de arquitecturas de redes neuronales mediante **algoritmos gen√©ticos**. El objetivo es encontrar la mejor arquitectura de red para clasificar d√≠gitos del dataset MNIST.

---
## Ejemplo 1
## Contenido del Ejemplo 1 de Neuroevolution

- `MNIST.py` : Script principal que implementa el algoritmo gen√©tico para hallar la mejor arquitectura.  
- `mejor_modelo_5.h5` : Modelos guardados durante el entrenamiento (uno por mejora encontrada).  
- `README.md` : Documentaci√≥n y gu√≠a de ejecuci√≥n del proyecto.

---

## Funcionamiento del C√≥digo

El c√≥digo est√° organizado en varias secciones, cada una con un prop√≥sito espec√≠fico:

### 1. Configuraci√≥n

```python
activaciones = ["relu", "tanh", "sigmoid", "elu", "gelu"]
tam_poblacion = 12
generaciones = 7
epocas_entrenamiento = 10

    -Define las funciones de activaci√≥n posibles.
    -Configura el tama√±o de la poblaci√≥n, el n√∫mero de generaciones y las √©pocas de entrenamiento de cada red.
```
### 2. Dataset
```
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 28*28).astype("float32") / 255
x_test = x_test.reshape(-1, 28*28).astype("float32") / 255

-Se carga MNIST y se normalizan los p√≠xeles entre 0 y 1.
-Se aplana cada imagen de 28x28 a un vector de 784 caracter√≠sticas.
```
### 3. Representaci√≥n del Cromosoma
```
def crear_cromosoma():
    num_capas = random.randint(1, 5)
    neuronas = [random.choice([32, 64, 128, 256]) for _ in range(num_capas)]
    activacion = random.choice(activaciones)
    return [num_capas, neuronas, activacion]

-Cada cromosoma representa una arquitectura:
[n√∫mero de capas, lista de neuronas por capa, funci√≥n de activaci√≥n].
```
### 4. Evaluaci√≥n del Cromosoma
```
def evaluar_cromosoma(cromosoma, guardar_mejor=False, nombre_modelo="mejor_modelo.h5"):
    ...

-Se construye un modelo secuencial seg√∫n el cromosoma.
-Se entrena brevemente (EarlyStopping incluido) y se obtiene la precisi√≥n de validaci√≥n.
-Si es el mejor modelo hasta el momento, se guarda en disco (.h5).
```
### 5. Selecci√≥n
```
def seleccionar(poblacion, fitness, k=3):
    indices = np.argsort(fitness)[-k:]  # mejores k
    return [poblacion[i] for i in indices]

-Se seleccionan las k mejores arquitecturas de la generaci√≥n actual.
-Esto implementa elitismo, asegurando que los mejores cromosomas sobrevivan.
```
### 6. Cruzamiento
```
def cruzar(padre1, padre2):
    ...

-Combina dos cromosomas para generar un hijo:
-Puede intercambiar las capas de neuronas o la funci√≥n de activaci√≥n.
-Permite explorar nuevas combinaciones de arquitecturas.
```
### 7. Mutaci√≥n
```
def mutar(cromosoma):
    ...

-Cambia aleatoriamente partes del cromosoma:
    -N√∫mero de neuronas por capa.
    -Funci√≥n de activaci√≥n.
    -N√∫mero de capas.
-Introduce diversidad gen√©tica, evitando convergencia prematura.
```
### 8. Algoritmo Gen√©tico
```
poblacion = [crear_cromosoma() for _ in range(tam_poblacion)]
...

Se inicializa una poblaci√≥n de cromosomas aleatorios.
Por cada generaci√≥n:
    1. Se eval√∫a cada cromosoma y se guarda su fitness.
    2. Se identifica el mejor cromosoma de la generaci√≥n.
    3. Se actualiza el mejor global y se guarda el modelo.
    4. Se seleccionan los mejores para reproducirse.
    5. Se generan nuevos cromosomas mediante cruzamiento y mutaci√≥n.
Se repite durante todas las generaciones, optimizando la arquitectura.
```
### 9. Evaluaci√≥n Final
```
best_model = keras.models.load_model(f"mejor_modelo_{contador_modelo}.h5")
loss, acc = best_model.evaluate(x_test, y_test, verbose=0)

-Se carga el mejor modelo guardado.
-Se eval√∫a en el set de test para obtener la precisi√≥n final.
```
### 10. Visualizaci√≥n
```
plt.plot(mejores_fitness, marker='o')

-Grafica la evoluci√≥n del fitness (accuracy) por generaci√≥n.
-Permite observar c√≥mo mejora la poblaci√≥n a lo largo del tiempo.
```
### Resultados Obtenidos
```
-Mejor arquitectura: [2, [256], 'gelu']
-Fitness m√°ximo (val_accuracy): 0.9824
-Accuracy en test: 0.9805
```

### Ejemplo 2:
```
C. Uso de algoritmos gen√©ticos para hallar la mejor arquitectura de una red neuronal o ‚ÄúNeuroevolution‚Äù.

üß¨ Optimizaci√≥n de Redes Neuronales con Algoritmo Gen√©tico (Titanic Dataset)
Este proyecto implementa un algoritmo gen√©tico (AG) para optimizar la arquitectura de una red neuronal profunda aplicada al famoso dataset del Titanic.
El objetivo es encontrar autom√°ticamente la mejor combinaci√≥n de capas y neuronas que maximicen la precisi√≥n en la tarea de predicci√≥n de supervivencia.

üìÇ Contenido del proyecto
AG_neuroevolution.py ‚Üí C√≥digo principal con preprocesamiento, AG y entrenamiento de modelos.
evolucion_accuracy.png ‚Üí Gr√°fico de la evoluci√≥n de accuracy del mejor individuo por generaci√≥n.
curva_accuracy_mejor_modelo.png ‚Üí Curvas de accuracy (train vs val) del mejor modelo.
curva_loss_mejor_modelo.png ‚Üí Curvas de p√©rdida (train vs val) del mejor modelo.
‚öôÔ∏è Tecnolog√≠as utilizadas
Python 3.10+
TensorFlow/Keras ‚Üí Construcci√≥n y entrenamiento de redes neuronales.
Scikit-learn ‚Üí Preprocesamiento de datos.
Matplotlib ‚Üí Visualizaci√≥n de m√©tricas y evoluci√≥n del AG.
NumPy & Pandas ‚Üí Manejo de datos.
Algoritmo gen√©tico propio implementado desde cero.
üìä Dataset: Titanic
Se utiliza el dataset cl√°sico de sobrevivientes del Titanic.
Puedes descargarlo desde Kaggle: Titanic Dataset
o usar cualquier versi√≥n CSV equivalente.

El dataset debe contener al menos las siguientes columnas:

Survived (etiqueta, 0 = no sobrevivi√≥, 1 = sobrevivi√≥)
Pclass (clase del pasajero)
Sex
Age
SibSp
Parch
Fare
Embarked
En el c√≥digo, columnas irrelevantes como Name, Ticket, Cabin, PassengerId son eliminadas.

üß© Estructura del algoritmo gen√©tico
Inicializaci√≥n

Se crea una poblaci√≥n de arquitecturas aleatorias.
Cada arquitectura est√° representada como una lista de enteros ‚Üí n√∫mero de neuronas por capa oculta.
Ejemplo: [32, 16] significa 2 capas ocultas con 32 y 16 neuronas respectivamente.
Fitness (Evaluaci√≥n)

Cada arquitectura se transforma en un modelo Keras.
Se entrena con los datos de entrenamiento.
El fitness se mide como la accuracy de validaci√≥n.
Selecci√≥n

Se eligen los 2 mejores individuos de cada generaci√≥n (elitismo).
Crossover (Reproducci√≥n)

Se combina parte de la arquitectura de un padre con parte de otro.
Mutaci√≥n

Se altera aleatoriamente el n√∫mero de neuronas en alguna capa.
Nueva poblaci√≥n

Se construye una nueva generaci√≥n con padres + hijos.
Iteraci√≥n

El proceso se repite hasta alcanzar el n√∫mero de generaciones definidas.
üìà Resultados
Se generan gr√°ficos para interpretar los resultados:
Evoluci√≥n del accuracy a lo largo de generaciones.
Curvas de entrenamiento (accuracy y p√©rdida) del mejor modelo.
Ejemplo de salida en consola:

=== Generaci√≥n 1 ===Arquitectura: [42, 17] | Accuracy: 0.7989Arquitectura: [21] | Accuracy: 0.7854...Padres seleccionados: [[42, 17], [21]] === Mejor arquitectura encontrada ===Arquitectura: [42, 17] | Accuracy: 0.8021

üöÄ Ejecuci√≥n en Google Colab
Sube el dataset Titanic a tu Google Drive.
Monta el Drive en Colab:
from google.colab import drive
drive.mount('/content/drive')

3. Ajusta la ruta del dataset en el c√≥digo:

titanic = pd.read_csv("/content/drive/MyDrive/Titanic-Dataset.csv")

4. Ejecuta todo el script.