{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unPAx3LsDbTL",
        "outputId": "19a5ecb7-852f-4376-9639-667e50eeb94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deap) (2.0.2)\n",
            "Downloading deap-1.4.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/136.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deap\n",
            "Successfully installed deap-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Librerías para el Algoritmo Genético\n",
        "import random\n",
        "from deap import base, creator, tools, algorithms\n",
        "\n",
        "# --- 1. Carga y Preparación de Datos (similar a tu código) ---\n",
        "# Usamos un transformador para convertir las imágenes a tensores de PyTorch\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Descargar y cargar los datasets\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Extraer todos los datos a la memoria para un acceso más rápido durante la HPO\n",
        "# Tu código original hacía esto al usar un DataLoader con el tamaño completo del dataset.\n",
        "# Lo mantenemos así para ser consistentes.\n",
        "x_train = train_dataset.data.view(train_dataset.data.size(0), -1).float() / 255.0\n",
        "y_train = train_dataset.targets\n",
        "x_test = test_dataset.data.view(test_dataset.data.size(0), -1).float() / 255.0\n",
        "y_test = test_dataset.targets\n",
        "\n",
        "# --- NUEVO: Configuración del dispositivo (GPU o CPU) ---\n",
        "# Comprueba si hay una GPU disponible (CUDA) y la selecciona. Si no, usa la CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando el dispositivo: {device}\\n\")\n",
        "\n",
        "# Mueve los tensores de datos principales al dispositivo seleccionado una sola vez\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "# --- 2. Definición del Modelo (el mismo que tenías) ---\n",
        "class SoftmaxModel(nn.Module):\n",
        "    def __init__(self, input_size=784, num_classes=10):\n",
        "        super(SoftmaxModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# --- 3. Función de Aptitud (Fitness Function) ---\n",
        "# Esta es la función clave. El algoritmo genético la llamará para evaluar cada \"individuo\".\n",
        "# Un individuo representa una combinación de hiperparámetros.\n",
        "def evaluate_hyperparameters(individual):\n",
        "    \"\"\"\n",
        "    Entrena y evalúa el modelo con un conjunto de hiperparámetros (un individuo).\n",
        "    Devuelve la precisión en el conjunto de prueba como una tupla, que es la \"aptitud\".\n",
        "    \"\"\"\n",
        "    # Desempaquetar los hiperparámetros del individuo\n",
        "    # Gen 0: Tasa de aprendizaje (lr)\n",
        "    # Gen 1: Tamaño del lote (batch_size)\n",
        "    # Gen 2: Elección del optimizador (0 para Adam, 1 para SGD)\n",
        "    lr, batch_size_idx, optimizer_idx = individual\n",
        "\n",
        "    # Mapear los índices a valores reales\n",
        "    batch_size_options = [32, 64, 128, 256]\n",
        "    optimizer_options = ['Adam', 'SGD']\n",
        "\n",
        "    batch_size = batch_size_options[batch_size_idx]\n",
        "    optimizer_choice = optimizer_options[optimizer_idx]\n",
        "\n",
        "    # Crear el modelo y el criterio de pérdida\n",
        "    # Mueve el modelo al dispositivo seleccionado (GPU o CPU)\n",
        "    model = SoftmaxModel().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Seleccionar el optimizador basado en el gen del individuo\n",
        "    if optimizer_choice == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    else:\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    # Crear DataLoader para el entrenamiento con el batch_size del individuo\n",
        "    train_data = TensorDataset(x_train, y_train)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Entrenamiento (reducimos las épocas para que cada evaluación sea más rápida)\n",
        "    epochs = 20\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for data, targets in train_loader:\n",
        "            # Los datos ya están en el dispositivo correcto porque el TensorDataset se creó con tensores en el dispositivo\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluación\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # x_test ya está en el dispositivo, por lo que no es necesario moverlo de nuevo\n",
        "        test_outputs = model(x_test)\n",
        "        _, predicted = torch.max(test_outputs, 1)\n",
        "        accuracy = (predicted == y_test).float().mean().item()\n",
        "\n",
        "    # DEAP requiere que la aptitud se devuelva como una tupla\n",
        "    return (accuracy,)\n",
        "\n",
        "# --- 4. Configuración del Algoritmo Genético con DEAP ---\n",
        "\n",
        "# Definir el objetivo: maximizar la precisión (weights=(1.0,))\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "# Definir un individuo como una lista, con el objetivo de fitness definido arriba\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Definir los \"genes\": cómo se genera cada hiperparámetro aleatoriamente\n",
        "# Gen 0: Tasa de aprendizaje (un float entre 0.0001 y 0.1)\n",
        "toolbox.register(\"attr_lr\", random.uniform, 0.0001, 0.1)\n",
        "# Gen 1: Índice para el tamaño del lote (un entero entre 0 y 3)\n",
        "toolbox.register(\"attr_batch_size\", random.randint, 0, 3)\n",
        "# Gen 2: Índice para el optimizador (un entero entre 0 y 1)\n",
        "toolbox.register(\"attr_optimizer\", random.randint, 0, 1)\n",
        "\n",
        "# Definir un individuo: una combinación de los 3 genes\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "                 (toolbox.attr_lr, toolbox.attr_batch_size, toolbox.attr_optimizer), n=1)\n",
        "\n",
        "# Definir la población: una lista de individuos\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Registrar las operaciones del algoritmo genético\n",
        "toolbox.register(\"evaluate\", evaluate_hyperparameters)  # Función de aptitud\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)            # Cruce\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3) # Selección\n",
        "\n",
        "# Función de mutación personalizada para manejar diferentes tipos de datos\n",
        "def mutate_hyperparameters(individual, indpb):\n",
        "    # Mutar tasa de aprendizaje (float)\n",
        "    if random.random() < indpb:\n",
        "        individual[0] = random.uniform(0.0001, 0.1)\n",
        "    # Mutar tamaño del lote (índice entero)\n",
        "    if random.random() < indpb:\n",
        "        individual[1] = random.randint(0, 3)\n",
        "    # Mutar optimizador (índice entero)\n",
        "    if random.random() < indpb:\n",
        "        individual[2] = random.randint(0, 1)\n",
        "    return individual,\n",
        "\n",
        "toolbox.register(\"mutate\", mutate_hyperparameters, indpb=0.2) # Probabilidad de mutar cada gen\n",
        "\n",
        "# --- 5. Ejecución del Algoritmo Genético ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Parámetros del GA\n",
        "    POP_SIZE = 20  # Tamaño de la población\n",
        "    CXPB = 0.5     # Probabilidad de cruce\n",
        "    MUTPB = 0.2    # Probabilidad de mutación\n",
        "    NGEN = 10      # Número de generaciones\n",
        "\n",
        "    print(\"--- Iniciando la optimización de hiperparámetros con Algoritmo Genético ---\")\n",
        "\n",
        "    # Crear la población inicial\n",
        "    population = toolbox.population(n=POP_SIZE)\n",
        "\n",
        "    # Guardar el mejor individuo encontrado\n",
        "    hof = tools.HallOfFame(1)\n",
        "\n",
        "    # Configurar estadísticas para ver el progreso\n",
        "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "    stats.register(\"avg\", np.mean)\n",
        "    stats.register(\"std\", np.std)\n",
        "    stats.register(\"min\", np.min)\n",
        "    stats.register(\"max\", np.max)\n",
        "\n",
        "    # Ejecutar el algoritmo\n",
        "    algorithms.eaSimple(population, toolbox, cxpb=CXPB, mutpb=MUTPB, ngen=NGEN,\n",
        "                        stats=stats, halloffame=hof, verbose=True)\n",
        "\n",
        "    # --- 6. Resultados ---\n",
        "    print(\"\\n--- Optimización Finalizada ---\")\n",
        "    best_individual = hof[0]\n",
        "    best_params = {\n",
        "        \"learning_rate\": best_individual[0],\n",
        "        \"batch_size\": [32, 64, 128, 256][best_individual[1]],\n",
        "        \"optimizer\": ['Adam', 'SGD'][best_individual[2]]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nMejor individuo encontrado: {best_individual}\")\n",
        "    print(f\"Mejores hiperparámetros: {best_params}\")\n",
        "    print(f\"Mejor precisión de validación durante la HPO: {best_individual.fitness.values[0]:.4f}\")\n",
        "\n",
        "    # Opcional: Entrenar un modelo final con los mejores hiperparámetros y más épocas\n",
        "    print(\"\\n--- Entrenando modelo final con los mejores hiperparámetros ---\")\n",
        "    final_model = SoftmaxModel().to(device)\n",
        "    final_criterion = nn.CrossEntropyLoss()\n",
        "    if best_params['optimizer'] == 'Adam':\n",
        "        final_optimizer = optim.Adam(final_model.parameters(), lr=best_params['learning_rate'])\n",
        "    else:\n",
        "        final_optimizer = optim.SGD(final_model.parameters(), lr=best_params['learning_rate'])\n",
        "\n",
        "    final_train_data = TensorDataset(x_train, y_train)\n",
        "    final_train_loader = DataLoader(final_train_data, batch_size=best_params['batch_size'], shuffle=True)\n",
        "\n",
        "    final_epochs = 100 # Entrenamos por más tiempo con los mejores parámetros\n",
        "    final_model.train()\n",
        "    for epoch in range(final_epochs):\n",
        "        for data, targets in final_train_loader:\n",
        "            # Los datos ya están en el dispositivo correcto\n",
        "            final_optimizer.zero_grad()\n",
        "            outputs = final_model(data)\n",
        "            loss = final_criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            final_optimizer.step()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Época final [{epoch+1}/{final_epochs}], Pérdida: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluación final\n",
        "    final_model.eval()\n",
        "    with torch.no_grad():\n",
        "        final_outputs = final_model(x_test)\n",
        "        _, final_predicted = torch.max(final_outputs, 1)\n",
        "        final_accuracy = (final_predicted == y_test).float().mean().item()\n",
        "\n",
        "    print(f\"\\nPrecisión final en el conjunto de prueba: {final_accuracy:.4f}\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.12MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 133kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.10MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando el dispositivo: cuda\n",
            "\n",
            "--- Iniciando la optimización de hiperparámetros con Algoritmo Genético ---\n",
            "gen\tnevals\tavg    \tstd      \tmin   \tmax   \n",
            "0  \t20    \t0.91591\t0.0107891\t0.8851\t0.9246\n",
            "1  \t11    \t0.921165\t0.00546895\t0.9042\t0.926 \n",
            "2  \t15    \t0.924215\t0.000898479\t0.9217\t0.9258\n",
            "3  \t11    \t0.92477 \t0.000689277\t0.9232\t0.9258\n",
            "4  \t14    \t0.924325\t0.00212105 \t0.9167\t0.9265\n",
            "5  \t11    \t0.924715\t0.0023176  \t0.9153\t0.9267\n",
            "6  \t12    \t0.92469 \t0.00194728 \t0.9177\t0.9268\n",
            "7  \t14    \t0.924565\t0.00176274 \t0.9189\t0.9271\n",
            "8  \t12    \t0.92436 \t0.00283221 \t0.9128\t0.9268\n",
            "9  \t13    \t0.9249  \t0.00107331 \t0.9221\t0.9263\n",
            "10 \t11    \t0.923915\t0.00574642 \t0.8992\t0.9263\n",
            "\n",
            "--- Optimización Finalizada ---\n",
            "\n",
            "Mejor individuo encontrado: [0.08392657225847973, 0, 1]\n",
            "Mejores hiperparámetros: {'learning_rate': 0.08392657225847973, 'batch_size': 32, 'optimizer': 'SGD'}\n",
            "Mejor precisión de validación durante la HPO: 0.9267\n",
            "\n",
            "--- Entrenando modelo final con los mejores hiperparámetros ---\n",
            "Época final [10/100], Pérdida: 0.1467\n",
            "Época final [20/100], Pérdida: 0.1029\n",
            "Época final [30/100], Pérdida: 0.1052\n",
            "Época final [40/100], Pérdida: 0.5969\n",
            "Época final [50/100], Pérdida: 0.3509\n",
            "Época final [60/100], Pérdida: 0.0909\n",
            "Época final [70/100], Pérdida: 0.2185\n",
            "Época final [80/100], Pérdida: 0.1974\n",
            "Época final [90/100], Pérdida: 0.6782\n",
            "Época final [100/100], Pérdida: 0.2875\n",
            "\n",
            "Precisión final en el conjunto de prueba: 0.9248\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZODqjKxZ6y_l",
        "outputId": "2ed4eb45-4b45-468d-cf4d-f884136f6f97"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}